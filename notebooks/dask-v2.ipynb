{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T14:21:06.690430Z",
     "start_time": "2020-01-13T14:20:54.182849Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install jupyter_contrib_nbextensions\n",
    "pip install jupyter_nbextensions_configurator\n",
    "jupyter contrib nbextension install --user\n",
    "jupyter nbextensions_configurator enable --user\n",
    "\n",
    "jupyter nbextension enable codefolding/main\n",
    "jupyter nbextension enable scratchpad/main\n",
    "jupyter nbextension enable execute_time/ExecuteTime\n",
    "jupyter nbextension enable autosavetime/main\n",
    "pip install -U pip dask numpy fsspec>=0.3.3 tqdm pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T13:54:01.055209Z",
     "start_time": "2020-01-23T13:53:59.104116Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pip dask numpy fsspec>=0.3.3 tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive s3://xdss-public-datasets/demos/taxi_parquet ../datasets/taxi_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:43:28.801151Z",
     "start_time": "2020-01-25T14:43:28.797045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test for 1 repetitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from src.config import repetitions\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "name = 'dask'\n",
    "instance_type = 'mlm52xlarge'\n",
    "data_path = '../datasets/taxi_parquet/'\n",
    "results_path = f\"../results/{name}_1b_{instance_type}.csv\"\n",
    "benchmarks = {}\n",
    "print(f\"test for {repetitions} repetitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:45:07.030549Z",
     "start_time": "2020-01-25T14:45:07.025311Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "\n",
    "\n",
    "def get_results(benchmarks, name):\n",
    "    results = pd.DataFrame.from_dict(benchmarks, orient='index')\n",
    "    results.columns = [name]\n",
    "    return results\n",
    "\n",
    "def persist():\n",
    "    get_results(benchmarks, name).to_csv(results_path)\n",
    "    !aws s3 cp  ../results/dask_1b_mlm52xlarge.csv s3://vaex-sagemaker-demo/benchmarks/dask_1b_mlm52xlarge_results.csv \n",
    "    \n",
    "def benchmark(f, df, name, repetitions=1, **kwargs):\n",
    "    times = []\n",
    "    for i in range(repetitions):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        times.append(time.time()-start_time)\n",
    "    benchmarks[name] = np.mean(times)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks[name]}\")\n",
    "    return benchmarks[name]\n",
    "\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:43:14.621933Z",
     "start_time": "2020-01-25T13:43:13.823085Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "print(f\"size: {len(data)} with {len(data.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:45:10.433197Z",
     "start_time": "2020-01-25T14:45:09.032480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 577 Bytes/577 Bytes (11.2 KiB/s) with 1 file(s) remaining\r",
      "upload: ../results/dask_1b_mlm52xlarge.csv to s3://vaex-sagemaker-demo/benchmarks/dask_1b_mlm52xlarge_results.csv\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7684509754180908"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(data_path, engine='pyarrow')\n",
    "\n",
    "key = read_file_parquet\n",
    "f= read_file_parquet\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:46:24.352103Z",
     "start_time": "2020-01-25T14:45:35.519885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 576 Bytes/576 Bytes (7.2 KiB/s) with 1 file(s) remaining\r",
      "upload: ../results/dask_1b_mlm52xlarge.csv to s3://vaex-sagemaker-demo/benchmarks/dask_1b_mlm52xlarge_results.csv\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.17900013923645"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "key = 'count'\n",
    "f = count\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:47:58.573702Z",
     "start_time": "2020-01-25T14:47:44.484263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 577 Bytes/577 Bytes (12.5 KiB/s) with 1 file(s) remaining\r",
      "upload: ../results/dask_1b_mlm52xlarge.csv to s3://vaex-sagemaker-demo/benchmarks/dask_1b_mlm52xlarge_results.csv\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.460501909255981"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "key = 'mean'\n",
    "f = mean\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:47:44.481755Z",
     "start_time": "2020-01-25T14:47:29.867257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 576 Bytes/576 Bytes (7.9 KiB/s) with 1 file(s) remaining\r",
      "upload: ../results/dask_1b_mlm52xlarge.csv to s3://vaex-sagemaker-demo/benchmarks/dask_1b_mlm52xlarge_results.csv\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.961443424224854"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "key = 'standard deviation'\n",
    "f = standard_deviation\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the time when using two columns, we can't return the response since it will get into memroy and break, so we run a mean calculation on it, and then remove the time it took to run the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.trip_distance).mean().compute()\n",
    "\n",
    "key = 'sum columns mean'\n",
    "f = mean_of_sum\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)\n",
    "benchmarks['sum columns'] =  benchmarks['sum columns mean'] - benchmarks['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T12:46:15.298677Z",
     "start_time": "2020-01-25T12:45:58.666423Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.trip_distance).mean().compute()\n",
    "\n",
    "key = 'product columns mean'\n",
    "f = mean_of_product\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)\n",
    "benchmarks['product columns'] =  benchmarks['product columns mean'] - benchmarks['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T12:49:16.297072Z",
     "start_time": "2020-01-25T12:46:15.300207Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "key = 'arithmetic operation mean'\n",
    "f  = mean_of_complicated_arithmetic_operation\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)\n",
    "benchmarks['arithmetic operation'] =  benchmarks['arithmetic operation mean'] - benchmarks['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T12:49:30.727079Z",
     "start_time": "2020-01-25T12:49:16.298605Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "key = 'value counts'\n",
    "f  = value_counts\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-25T12:51:30.334Z"
    }
   },
   "outputs": [],
   "source": [
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], \n",
    "                                               'tip_amount': ['mean', 'std']\n",
    "                                              }).compute()\n",
    "\n",
    "key = 'groupby statistics'\n",
    "f = groupby_statistics\n",
    "benchmark(f, df=data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T14:06:29.116231Z",
     "start_time": "2020-01-25T13:43:20.831441Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "other = groupby_statistics(data)\n",
    "key = 'join count'\n",
    "f = join_count\n",
    "benchmark(f, data, name=key, repetitions=repetitions, other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will crash\n",
    "benchmarks['join'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is not build to run on filter data like you would normally, so we will apply the same strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:36:34.383105Z",
     "start_time": "2020-01-25T13:36:34.328062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare filtered data and deleted 798 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prepare filtered data and deleted {gc.collect()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:36:52.817651Z",
     "start_time": "2020-01-25T13:36:52.799756Z"
    }
   },
   "outputs": [],
   "source": [
    "expr_filter = (data.pickup_longitude > long_min)  & (data.pickup_longitude < long_max) & \\\n",
    "                  (data.pickup_latitude > lat_min)    & (data.pickup_latitude < lat_max) & \\\n",
    "                  (data.dropoff_longitude > long_min) & (data.dropoff_longitude < long_max) & \\\n",
    "                  (data.dropoff_latitude > lat_min)   & (data.dropoff_latitude < lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:36:58.356833Z",
     "start_time": "2020-01-25T13:36:58.312080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned 181 mb\n"
     ]
    }
   ],
   "source": [
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "key = 'filter data'\n",
    "f = filter_data\n",
    "benchmark(f, data, name=key, repetitions=repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd = filter_data(data)\n",
    "\n",
    "del data\n",
    "print(f\"cleaned {gc.collect()} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(filter_data, filterd, name='filterd count', repetitions=repetitions)\n",
    "benchmark(mean, filterd, name='filterd mean', repetitions=repetitions)\n",
    "benchmark(standard_deviation, filterd, name='filtered standard deviation', repetitions=repetitions)\n",
    "benchmark(mean_of_sum, filterd, name ='filtered sum columns mean', repetitions=repetitions)\n",
    "benchmarks['filtered sum columns'] =  benchmarks['filtered sum columns mean'] - benchmarks['filterd mean']\n",
    "benchmark(mean_of_product, filterd, name ='filterd product columns mean', repetitions=repetitions)\n",
    "benchmarks['filterd product columns'] = benchmarks['filterd product columns mean'] - benchmarks['filterd mean']\n",
    "benchmark(mean_of_complicated_arithmetic_operation, filterd, name='filterd arithmetic operation mean', repetitions=repetitions)\n",
    "benchmarks['filterd arithmetic operation'] =  benchmarks['filterd arithmetic operation mean'] - benchmarks['filterd mean']\n",
    "benchmark(value_counts, filterd, name ='filtered value counts', repetitions=repetitions)\n",
    "benchmark(groupby_statistics, filterd, name='filtered groupby statistics', repetitions=repetitions)\n",
    "other = groupby_statistics(data)\n",
    "benchmarks['filtered join'] = -1\n",
    "benchmarks['filtered join count'] = benchmark(join_count, filterd, repetitions=repetitions, other=other)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
