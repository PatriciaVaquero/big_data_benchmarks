{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T20:04:30.087591Z",
     "start_time": "2020-05-27T20:04:28.323364Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install -U pip dask[complete] numpy fsspec>=0.3.3 tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:29.030635Z",
     "start_time": "2020-05-27T21:27:28.723400Z"
    },
    "code_folding": [
     16,
     31,
     36,
     47
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We test every benchmark twice and save both results\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "print(f\"We test every benchmark twice and save both results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:41.524308Z",
     "start_time": "2020-05-27T21:27:30.093794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "# I found that running this line makes for better results\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:43.737168Z",
     "start_time": "2020-05-27T21:27:41.531161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_file took: 0.8108847141265869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8108847141265869"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(data_path, engine='pyarrow')\n",
    "\n",
    "benchmark(read_file_parquet, df=data, name='read_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:00.078754Z",
     "start_time": "2020-05-27T21:27:43.738756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count took: 7.938486099243164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.938486099243164"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(df=None):\n",
    "    # there is a dask bug - len(df) takes 20X time longer\n",
    "    return len(df.vendor_id) \n",
    "\n",
    "benchmark(count, df=data, name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:11.395255Z",
     "start_time": "2020-05-27T21:28:00.080420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean took: 5.056631803512573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.056631803512573"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "benchmark(mean, df=data, name='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:22.583181Z",
     "start_time": "2020-05-27T21:28:11.397022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation took: 5.378662586212158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.378662586212158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "benchmark(standard_deviation, df=data, name='standard deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:36.977654Z",
     "start_time": "2020-05-27T21:28:22.584840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum columns mean took: 6.816420078277588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.816420078277588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.trip_distance).mean().compute()\n",
    "\n",
    "benchmark(mean_of_sum, df=data, name='sum columns mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:37.560475Z",
     "start_time": "2020-05-27T21:28:36.980044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum columns took: 0.0010232925415039062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0010232925415039062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lazy evaulation - instant\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.trip_distance)\n",
    "\n",
    "benchmark(sum_columns, df=data, name='sum columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:51.518685Z",
     "start_time": "2020-05-27T21:28:37.562329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product columns mean took: 6.626446723937988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.626446723937988"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.trip_distance).mean().compute()\n",
    "\n",
    "benchmark(mean_of_product, df=data, name='product columns mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:52.144439Z",
     "start_time": "2020-05-27T21:28:51.520316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product columns took: 0.0011837482452392578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0011837482452392578"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lazy evaulation - instant\n",
    "def product_columns(df):\n",
    "    return df.fare_amount * df.trip_distance\n",
    "\n",
    "benchmark(product_columns, df=data, name='product columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:33:16.210086Z",
     "start_time": "2020-05-27T21:29:00.083187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lazy evaluation took: 153.01910591125488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "153.01910591125488"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lazy_mean(df):\n",
    "    df['lazy'] = df.fare_amount * df.trip_distance\n",
    "    return df['lazy'].mean().compute()\n",
    "    \n",
    "benchmark(lazy_mean, df=data, name='lazy evaluation')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart and reload the data is much faster... \n",
    "I got results between 4X - 75X faster by using this.    \n",
    "`client.restart()` did not match the effect of retarting the kenrel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:36:15.005842Z",
     "start_time": "2020-05-27T21:36:03.197324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:36:26.329649Z",
     "start_time": "2020-05-27T21:36:15.033436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value counts took: 5.161530494689941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.161530494689941"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_counts(df):\n",
    "    return df.passenger_count.value_counts().compute()\n",
    "\n",
    "benchmark(value_counts, df=data, name='value counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:39:06.898262Z",
     "start_time": "2020-05-27T21:37:52.565642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic operation mean took: 36.519623041152954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.519623041152954"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=data, name='arithmetic operation mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:39:07.516482Z",
     "start_time": "2020-05-27T21:39:06.899736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic operation took: 0.019608259201049805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.019608259201049805"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "benchmark(complicated_arithmetic_operation, df=data, name='arithmetic operation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart for best results\n",
    "This restart got me a 7X faster results for group-by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:22:25.677178Z",
     "start_time": "2020-05-27T21:22:14.154146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:45:28.725243Z",
     "start_time": "2020-05-27T21:42:00.896969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby statistics took: 104.00652551651001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104.00652551651001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], \n",
    "                                               'tip_amount': ['mean', 'std']\n",
    "                                              }).compute()\n",
    "\n",
    "benchmark(groupby_statistics, df=data, name='groupby statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For join before restarting\n",
    "other = data.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], 'tip_amount': ['mean', 'std']}).compute()\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "other.to_parquet('datasets/other.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart again was the only way to finish this task\n",
    "* Removing this line `client = Client(threads_per_worker=1)` is also important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Join best practice](https://docs.dask.org/en/latest/dataframe-best-practices.html#joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:16:03.825859Z",
     "start_time": "2020-05-27T22:15:19.304783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "#client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "other = pd.read_parquet('datasets/other.parquet')\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:37:38.150121Z",
     "start_time": "2020-05-27T22:16:14.158180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join count took: 646.4160277843475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646.4160277843475"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "benchmark(join_count, data, name='join count', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:38:19.188195Z",
     "start_time": "2020-05-27T22:38:18.624049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join took: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crashes\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True)\n",
    "\n",
    "add_nan('join')\n",
    "# benchmark(join_count, data, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered data (and restart the kernel again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:38:49.294736Z",
     "start_time": "2020-05-27T22:38:36.612300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "          \n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.trip_distance).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.trip_distance)\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.trip_distance).mean().compute()\n",
    "\n",
    "\n",
    "def product_columns(df):\n",
    "    return df.fare_amount * df.trip_distance\n",
    "          \n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.passenger_count.value_counts().compute()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], \n",
    "                                               'tip_amount': ['mean', 'std']\n",
    "                                              }).compute()\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "          \n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "other = pd.read_parquet('datasets/other.parquet')\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:39:50.234262Z",
     "start_time": "2020-05-27T22:39:49.545991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare filtered data and deleted 388 MB\n",
      "filter data took: 0.00021028518676757812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00021028518676757812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Prepare filtered data and deleted {gc.collect()} MB\")\n",
    "expr_filter = (data.pickup_longitude > long_min)  & (data.pickup_longitude < long_max) & \\\n",
    "                  (data.pickup_latitude > lat_min)    & (data.pickup_latitude < lat_max) & \\\n",
    "                  (data.dropoff_longitude > long_min) & (data.dropoff_longitude < long_max) & \\\n",
    "                  (data.dropoff_latitude > lat_min)   & (data.dropoff_latitude < lat_max)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "benchmark(filter_data, data, name='filter data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:42:22.237665Z",
     "start_time": "2020-05-27T22:40:23.840931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned 248 mb\n"
     ]
    }
   ],
   "source": [
    "# https://docs.dask.org/en/latest/dataframe-best-practices.html\n",
    "filtered = filter_data(data)\n",
    "nb_partitions = int(data.npartitions//(len(filtered.vendor_id)/size))\n",
    "\n",
    "filtered = filtered.repartition(npartitions=nb_partitions)#.persist()\n",
    "del data\n",
    "print(f\"cleaned {gc.collect()} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-27T22:50:36.757Z"
    }
   },
   "outputs": [],
   "source": [
    "benchmark(mean, filtered, name='filtered mean')\n",
    "benchmark(standard_deviation, filtered, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, filtered, name ='filtered sum columns mean')\n",
    "benchmark(sum_columns, df=filtered, name='filtered sum columns')\n",
    "benchmark(mean_of_product, filtered, name ='filtered product columns mean')\n",
    "benchmark(product_columns, df=filtered, name='filtered product columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, filtered, name='filtered arithmetic operation mean')\n",
    "benchmark(complicated_arithmetic_operation, filtered, name='filtered arithmetic operation')\n",
    "benchmark(value_counts, filtered, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, filtered, name='filtered groupby statistics')\n",
    "other = filtered.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], 'tip_amount': ['mean', 'std']}).compute()\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "add_nan('filtered join')\n",
    "benchmark(join_count, filtered, name='filtered join count', other=other)\n",
    "print(name)\n",
    "get_results(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
